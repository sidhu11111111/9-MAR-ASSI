{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5e465c4f",
   "metadata": {},
   "source": [
    "#QNO.1 ANS\n",
    "The Probability Mass Function (PMF) is a function that gives the probability of a discrete random variable taking a specific value. It is defined as P(X=x), where X is the random variable and x is a specific value.\n",
    "For example, consider a coin flip. If X represents the number of heads obtained in two flips, then the PMF of X is given by:\n",
    "P(X=0) = 1/4\n",
    "P(X=1) = 1/2\n",
    "P(X=2) = 1/4\n",
    "\n",
    "The Probability Density Function (PDF) is a function that gives the probability density at a given point for a continuous random variable. It is defined as f(x), where x is a specific point. The probability of an event happening within a certain range of values is the area under the curve of the PDF between those values.\n",
    "For example, consider the height of individuals in a population. The PDF of heights could be a normal distribution, which has a bell-shaped curve with a mean and standard deviation. The PDF would give the probability density at any specific height, but the probability of someone being between a range of heights would be the area under the curve between those heights."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b3a9cf3",
   "metadata": {},
   "source": [
    "#QNO.2 ANS\n",
    "The Cumulative Density Function (CDF) is a function that gives the probability that a random variable is less than or equal to a specific value. It is defined as F(x), where x is a specific value. The CDF is a cumulative function, which means it adds up the probabilities of all values up to x.\n",
    "For example, consider a fair dice. The CDF of rolling a number less than or equal to 3 is given by:\n",
    "F(x<=3) = P(X=1) + P(X=2) + P(X=3) = 1/6 + 1/6 + 1/6 = 1/2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40b9a167",
   "metadata": {},
   "source": [
    "#QNO.3 ANS\n",
    "The normal distribution is commonly used to model continuous random variables in situations where the data is symmetrical and the majority of observations are clustered around the mean. Examples include the height or weight of individuals in a population, the scores on a standardized test, or the time taken for a manufacturing process to complete.\n",
    "\n",
    "The parameters of the normal distribution are the mean (µ) and the standard deviation (σ). The mean determines the central tendency of the distribution, while the standard deviation determines the spread of the distribution. A larger standard deviation indicates a more spread-out distribution, while a smaller standard deviation indicates a more concentrated distribution.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8d2d968",
   "metadata": {},
   "source": [
    "#QNO.4 ANS\n",
    "The normal distribution is important in statistics because it is a very common distribution that arises in many real-world situations. Some examples of real-life phenomena that can be modeled by a normal distribution include the distribution of heights or weights of people, the IQ scores of a population, and the errors in measurements or experimental results.\n",
    "\n",
    "The normal distribution is useful because it allows us to make predictions and draw conclusions about a population based on a sample. It also provides a baseline for comparing the distribution of other data sets. Additionally, many statistical tests and methods are based on the assumption of normality."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8faa60f",
   "metadata": {},
   "source": [
    "#QNO.5 ANS\n",
    "The Bernoulli distribution is a discrete probability distribution that models a single trial with two possible outcomes: success or failure. It is defined by a single parameter p, which represents the probability of success. The probability mass function of the Bernoulli distribution is given by:\n",
    "P(X=1) = p\n",
    "P(X=0) = 1-p\n",
    "\n",
    "An example of a Bernoulli distribution is a coin flip, where success is defined as getting heads and failure is defined as getting tails.\n",
    "\n",
    "The main difference between the Bernoulli distribution and the binomial distribution is that the binomial distribution models the number of successes in a fixed number of independent trials, each with the same probability of success. In other words, the binomial distribution is the sum of multiple independent Bernoulli trials. The binomial distribution is defined by two parameters: n, the number of trials, and p, the probability of success in each trial.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05b6ac43",
   "metadata": {},
   "source": [
    "#QNO.6 ANS\n",
    "Mean = 50 \n",
    "std deviation = 10\n",
    "we use this formula :\n",
    "z = (x - µ) / σ\n",
    "\n",
    "put the value\n",
    "z = (60-50)/10 = 1\n",
    "Now using standarad normal table \n",
    "\n",
    "probablity of a randomly selected observation being greater than 60 to be approx 15.87%."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6135bbf4",
   "metadata": {},
   "source": [
    "#QNO.7 ANS\n",
    "The uniform distribution is a continuous probability distribution that models a situation where all values in a given interval have an equal chance of being selected. The probability density function of the uniform distribution is constant within the interval and zero outside the interval. An example of a uniform distribution is the random selection of a number between 1 and 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcaea4db",
   "metadata": {},
   "source": [
    "#QNO.8 ANS\n",
    "The z-score, also known as the standard score, is a standardized value that represents the distance between a raw score and the mean in terms of standard deviations. It is calculated using the formula:\n",
    "z = (x - µ) / σ\n",
    "\n",
    "The z-score is important because it allows us to compare scores from different distributions that have different means and standard deviations. By standardizing scores, we can determine how extreme or unusual a particular score is in relation to the rest of the distribution. Z-scores are also used to calculate probabilities and confidence intervals"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9240002c",
   "metadata": {},
   "source": [
    "#QNO.9 ANS\n",
    "The Central Limit Theorem (CLT) is a statistical principle that states that as the sample size increases, the distribution of the sample means approaches a normal distribution, regardless of the underlying distribution of the population. This means that if we take multiple random samples from a population and calculate the mean of each sample, the distribution of those sample means will be approximately normal.\n",
    "\n",
    "using the CLT, we can estimate the population mean and determine the probability of obtaining a certain sample mean. This principle is widely used in hypothesis testing, confidence intervals, and other statistical methods."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f43af45",
   "metadata": {},
   "source": [
    "#QNO.10 ANS\n",
    "The assumptions of the Central Limit Theorem are:\n",
    "\n",
    "1.The sample is a random sample from the population.\n",
    "2.The sample size is large enough (typically n ≥ 30) to ensure that the sample mean is normally distributed.\n",
    "3.The observations in the sample are independent of each other.\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
